# ğŸ¦™ Tiny-LLaMA-From-Scratch (HappyLLM Implementation)

> **æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªåŸºäº PyTorch åŸç”Ÿä»£ç çš„å¤§è¯­è¨€æ¨¡å‹å¤ç°å·¥ç¨‹ã€‚**
> æ—¨åœ¨ä»é›¶å¼€å§‹ï¼ˆFrom Scratchï¼‰æ„å»ºä¸€ä¸ªæ¶æ„å¯¹é½ LLaMA çš„ Transformer æ¨¡å‹ï¼Œå¹¶åœ¨å•å¡ RTX 5070 (12GB) ä¸Šå®Œæˆäº†ä» Tokenizer è®­ç»ƒã€é¢„è®­ç»ƒ (Pretrain) åˆ°æŒ‡ä»¤å¾®è°ƒ (SFT) çš„å…¨æµç¨‹é—­ç¯ã€‚

<div align="center">

![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)
![Python](https://img.shields.io/badge/Python-3.12-green.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.9-orange.svg)

</div>

## âš ï¸ é¡¹ç›®æ€§è´¨è¯´æ˜ (Disclaimer)

> **æœ¬é¡¹ç›®ä¸ºä¸ªäººå­¦ä¹ æ€§è´¨çš„å¤ç°å·¥ç¨‹ (Educational Purpose)ã€‚**
>
> ä¸ºäº†å¿«é€ŸéªŒè¯ä»£ç ç®¡çº¿ä¸æ¶æ„çš„æ­£ç¡®æ€§ï¼Œæ¨¡å‹ä»…ä½¿ç”¨ **10,000 æ¡ (10k)** æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚å› æ­¤ï¼Œæ¨¡å‹**ä¸å…·å¤‡**å®é™…çš„å¯¹è¯æ™ºèƒ½æˆ–é€»è¾‘æ¨ç†èƒ½åŠ›ï¼ˆå¯èƒ½ä¼šå‡ºç°å¤è¯»ã€é€»è¾‘ä¸é€šç­‰ç°è±¡ï¼‰ã€‚
>
> **æœ¬é¡¹ç›®çš„æ ¸å¿ƒä»·å€¼åœ¨äºï¼š** è·‘é€šå¤§æ¨¡å‹ä» 0 åˆ° 1 çš„å®Œæ•´ä»£ç æµç¨‹ï¼Œæ·±å…¥ç†è§£ Transformer åº•å±‚ç»†èŠ‚ä¸è®­ç»ƒæœºåˆ¶ã€‚

## ğŸŒŸ é¡¹ç›®äº®ç‚¹ (Key Features)

æœ¬é¡¹ç›®ä¸ä¾èµ– `transformers` é«˜å±‚åº“çš„ç°æˆæ¨¡å‹æ¥å£ï¼Œè€Œæ˜¯é€šè¿‡ `torch.nn` æ‰‹å†™å®ç°äº†ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼Œç”¨ä»¥æ·±å…¥ç†è§£ LLM åº•å±‚åŸç†ï¼š

* **æ ¸å¿ƒæ¶æ„ (Model Architecture)**:
    * **RMSNorm**: ç›¸æ¯” LayerNorm è®¡ç®—æ›´é«˜æ•ˆçš„å½’ä¸€åŒ–å±‚ã€‚
    * **SwiGLU**: LLaMA æ ‡å¿—æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œå¢å¼ºæ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ã€‚
    * **RoPE (Rotary Positional Embeddings)**: å®ç°äº†æ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€šè¿‡å¤æ•°æ—‹è½¬çŸ©é˜µæ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œæ›´å¥½åœ°å¤„ç†é•¿æ–‡æœ¬åºåˆ—ã€‚
    * **GQA (Grouped Query Attention)**: å®ç°äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼ˆé€»è¾‘æ”¯æŒï¼‰ï¼Œä¸º KV Cache æ˜¾å­˜ä¼˜åŒ–æ‰“ä¸‹åŸºç¡€ã€‚
* **è®­ç»ƒç®¡çº¿ (Training Pipeline)**:
    * **Tokenizer**: åŸºäº SentencePiece è®­ç»ƒäº†ä¸“å±çš„ä¸­æ–‡åˆ†è¯å™¨ (BPE)ã€‚
    * **Pretrain**: å®ç°äº†æ ‡å‡†çš„ Next Token Prediction é¢„è®­ç»ƒä»»åŠ¡ã€‚
    * **SFT (Supervised Fine-Tuning)**: å®ç°äº†å¸¦æœ‰ **Loss Masking** æœºåˆ¶çš„æŒ‡ä»¤å¾®è°ƒï¼Œé€šè¿‡ `ignore_index=-100` å±è”½ Prompt éƒ¨åˆ†çš„æ¢¯åº¦ï¼Œå¼ºåˆ¶æ¨¡å‹ä¸“æ³¨äº Answer ç”Ÿæˆã€‚

## ğŸ“‚ ç›®å½•ç»“æ„ (Directory Structure)

```text
.
â”œâ”€â”€ checkpoints/          # (å·²å¿½ç•¥) é¢„è®­ç»ƒæ¨¡å‹æƒé‡å­˜æ¡£
â”œâ”€â”€ checkpoints_sft/      # (å·²å¿½ç•¥) SFT å¾®è°ƒåæ¨¡å‹æƒé‡å­˜æ¡£
â”œâ”€â”€ data/                 # (å·²å¿½ç•¥) å­˜æ”¾ jsonl æ•°æ®é›†
â”œâ”€â”€ model.py              # ã€æ ¸å¿ƒã€‘Transformerã€RMSNormã€RoPEã€Attention æ¶æ„å®ç°
â”œâ”€â”€ dataset.py            # é¢„è®­ç»ƒæ•°æ®å¤„ç† (Padding, Tokenization)
â”œâ”€â”€ dataset_sft.py        # SFT æ•°æ®å¤„ç† (Prompt Template, Loss Masking)
â”œâ”€â”€ train.py              # é¢„è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_sft.py          # SFT å¾®è°ƒè„šæœ¬ (åŠ è½½é¢„è®­ç»ƒæƒé‡ -> å¾®è°ƒ)
â”œâ”€â”€ inference.py          # åŸºç¡€æ¨¡å‹æ¨ç†è„šæœ¬
â”œâ”€â”€ inference_sft.py      # SFT å¯¹è¯æ¨¡å‹æ¨ç†è„šæœ¬ (å«å¯¹è¯æ¨¡æ¿)
â”œâ”€â”€ train_tokenizer.py    # åˆ†è¯å™¨è®­ç»ƒè„šæœ¬
â”œâ”€â”€ tokenizer.model       # è®­ç»ƒå¥½çš„åˆ†è¯å™¨äºŒè¿›åˆ¶æ–‡ä»¶
â”œâ”€â”€ requirements.txt      # é¡¹ç›®ä¾èµ–åº“åˆ—è¡¨
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜æ–‡æ¡£
``` 

## ğŸ› ï¸ å¿«é€Ÿå¼€å§‹ (Quick Start)

### 1. ç¯å¢ƒå‡†å¤‡

å»ºè®®åˆ›å»ºç‹¬ç«‹çš„ Conda ç¯å¢ƒï¼Œå¹¶å®‰è£…ä¾èµ–ï¼š

Bash

```
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡ä¸åˆ†è¯å™¨è®­ç»ƒ

ä¸‹è½½æµ‹è¯•æ•°æ®å¹¶è®­ç»ƒ SentencePiece åˆ†è¯å™¨ï¼š

Bash

```
# 1. ä¸‹è½½å°‘é‡æµ‹è¯•æ•°æ® (mini_data.jsonl)
python download_mini_data.py

# 2. è®­ç»ƒ Tokenizer (ç”Ÿæˆ tokenizer.model)
python train_tokenizer.py
```

### 3. é˜¶æ®µä¸€ï¼šé¢„è®­ç»ƒ (Pretraining)

ä»é›¶éšæœºåˆå§‹åŒ–æ¨¡å‹ï¼Œå­¦ä¹ è¯­è¨€çš„åŸºæœ¬æ¦‚ç‡åˆ†å¸ƒï¼š

Bash

```
python train.py
```

- *Output*: æ¨¡å‹æƒé‡å°†ä¿å­˜åœ¨ `checkpoints/` ç›®å½•ä¸‹ã€‚

### 4. é˜¶æ®µäºŒï¼šæŒ‡ä»¤å¾®è°ƒ (SFT)

åŠ è½½é¢„è®­ç»ƒå¥½çš„æƒé‡ï¼Œä½¿ç”¨å¯¹è¯æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œä½¿æ¨¡å‹å…·å¤‡æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼š

Bash

```
python train_sft.py
```

- *Key Tech*: æ­¤é˜¶æ®µåº”ç”¨äº† Mask æœºåˆ¶ï¼Œä¸è®¡ç®— "User" æé—®éƒ¨åˆ†çš„ Lossã€‚

### 5. æ¨ç†ä¸å¯¹è¯ (Inference)

ä¸å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œå¯¹è¯æµ‹è¯•ï¼š

Bash

```
python inference_sft.py
```

## ğŸ“Š å®éªŒç»“æœ (Results)

- **ç¡¬ä»¶ç¯å¢ƒ**: NVIDIA RTX 5070 (12GB VRAM)
- **è®­ç»ƒæ•ˆç‡**: åœ¨å•å¡ç¯å¢ƒä¸‹æˆåŠŸè·‘é€šå…¨æµç¨‹ï¼ŒéªŒè¯äº†å°å‚æ•°é‡æ¨¡å‹åœ¨ç‰¹å®šç¡¬ä»¶ä¸‹çš„å¯è®­ç»ƒæ€§ã€‚
- **SFT æ•ˆæœ**: æ¨¡å‹èƒ½å¤Ÿä¸¥æ ¼éµå¾ª `User: <query> \n AI: <response>` çš„å¯¹è¯æ¨¡æ¿æ ¼å¼è¿›è¡Œå›å¤ï¼ŒéªŒè¯äº† SFT æ•°æ®æ„é€  pipeline å’Œ Mask æœºåˆ¶çš„æ­£ç¡®æ€§ã€‚

## ğŸ”— å‚è€ƒèµ„æ–™ (References)

- **Paper**: [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- **Guide**: [Datawhale HappyLLM Project](https://github.com/datawhalechina/happy-llm)
- **Guide**ï¼šhttps://datawhalechina.github.io/happy-llm

## ğŸ™ è‡´è°¢ (Acknowledgements)

æ„Ÿè°¢ä»¥ä¸‹é¡¹ç›®å’Œç¤¾åŒºæä¾›çš„å­¦ä¹ èµ„æºï¼š

- æ„Ÿè°¢ **[Datawhale](https://github.com/datawhalechina)** æä¾›çš„å¼€æºæ•™ç¨‹ä¸ç¤¾åŒºæ”¯æŒã€‚
- æ„Ÿè°¢ **Meta AI** å¼€æºçš„ LLaMA æ¶æ„è®¾è®¡æ€è·¯ã€‚
- æ„Ÿè°¢ **PyTorch** ä¸ **SentencePiece** æä¾›çš„åº•å±‚å·¥å…·åº“æ”¯æŒã€‚

------

*Created by Tang Yuanhang for Learning Purpose.*

