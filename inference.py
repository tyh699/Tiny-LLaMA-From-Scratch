import torch
import sentencepiece as spm
from model import Transformer, ModelArgs
import os
import time

def main():
    # ========================================================
    # 1. åŸºç¡€é…ç½®
    # ========================================================
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # ç¡®ä¿åŠ è½½çš„æ˜¯æœ€æ–°è®­ç»ƒå¥½çš„ SFT æ¨¡å‹
    checkpoint_path = "./checkpoints_sft/model_sft_final.pth" 
    tokenizer_path = "./tokenizer.model"
    
    # æ¨¡å‹å‚æ•° (å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´)
    args = ModelArgs(
        dim=512,
        n_layers=8,
        n_heads=8,
        vocab_size=32000,
        max_seq_len=512
    )

    # ========================================================
    # 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨
    # ========================================================
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {checkpoint_path} ...")
    if not os.path.exists(checkpoint_path):
        print("âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    # åˆå§‹åŒ–æ¶æ„
    model = Transformer(args).to(device)
    # åŠ è½½æƒé‡
    state_dict = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(state_dict)
    # ã€é‡è¦ã€‘åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (å…³é—­ Dropout)
    model.eval()
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    # åŠ è½½åˆ†è¯å™¨
    sp = spm.SentencePieceProcessor()
    sp.load(tokenizer_path)
    print("âœ… åˆ†è¯å™¨åŠ è½½å®Œæˆï¼")

    # ========================================================
    # 3. æ ¸å¿ƒç”Ÿæˆå‡½æ•° (æ”¯æŒ KV Cache)
    # ========================================================
    def chat(prompt, temperature=0.8, max_len=100):
        """
        prompt: ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        temperature: é‡‡æ ·æ¸©åº¦ (è¶Šé«˜è¶Šå‘æ•£ï¼Œ0ä¸ºè´ªå©ªæœç´¢)
        max_len: æœ€å¤§ç”Ÿæˆé•¿åº¦
        """
        
        # --- A. æ„å»ºå¯¹è¯æ¨¡æ¿ ---
        # SFT æ¨¡å‹éœ€è¦è¿™ç§ç‰¹å®šçš„æ ¼å¼æ‰èƒ½å¬æ‡‚
        formatted_prompt = f"User: {prompt}\nAI: "
        
        # ç¼–ç å¹¶æ·»åŠ  BOS (Start Token)
        input_ids = sp.encode_as_ids(formatted_prompt)
        input_ids = [sp.bos_id()] + input_ids
        
        # è½¬ Tensorï¼Œå¹¶æ¬è¿åˆ° GPU
        # x åˆå§‹å½¢çŠ¶: (1, seq_len)
        x = torch.tensor(input_ids, dtype=torch.long).unsqueeze(0).to(device)
        
        # æ‰“å° AI å¼€å¤´ï¼Œå‡†å¤‡æµå¼è¾“å‡º
        print("AI: ", end="", flush=True)

        # --- B. åˆå§‹åŒ– KV Cache å˜é‡ ---
        # kv_caches åˆå§‹ä¸º Noneï¼Œæ¨¡å‹å†…éƒ¨ä¼šè‡ªåŠ¨åˆ›å»º
        kv_caches = None 
        # start_pos è®°å½•å½“å‰è¾“å…¥çš„èµ·å§‹ä½ç½® (ç”¨äº RoPE ä½ç½®ç¼–ç )
        start_pos = 0 
        
        start_time = time.time()
        new_tokens_count = 0

        # --- C. ç”Ÿæˆå¾ªç¯ ---
        with torch.no_grad(): # æ¨ç†æ¨¡å¼ä¸éœ€è¦ç®—æ¢¯åº¦
            for i in range(max_len):
                
                # [æ ¸å¿ƒé€»è¾‘] æ ¹æ®æ˜¯å¦æ˜¯ç¬¬ä¸€æ­¥ï¼Œå†³å®šå¦‚ä½•ä¼ å‚
                if i == 0:
                    # === Prefill é˜¶æ®µ (é¢„å¡«å……) ===
                    # ç¬¬ä¸€æ­¥æŠŠæ•´ä¸ª Prompt å–‚è¿›å»
                    # start_pos=0ï¼Œæ¨¡å‹ä¼šè®¡ç®—æ‰€æœ‰ token çš„ KV å¹¶å­˜å…¥ cache
                    logits, kv_caches = model(x, start_pos=0, kv_caches=None)
                    
                    # æ›´æ–° start_posï¼šç°åœ¨çš„é•¿åº¦å°±æ˜¯ä¸‹ä¸€æ¬¡çš„èµ·ç‚¹
                    start_pos = x.shape[1] 
                else:
                    # === Decode é˜¶æ®µ (è§£ç ) ===
                    # åç»­æ­¥éª¤åªå–‚è¿™ä¸€ä¸ªæ–°ç”Ÿæˆçš„å­— (last token)
                    # æ­¤æ—¶ x çš„å½¢çŠ¶å¿…é¡»æ˜¯ (1, 1)
                    # start_pos æ¯æ¬¡ +1
                    logits, kv_caches = model(x, start_pos=start_pos, kv_caches=kv_caches)
                    start_pos += 1

                # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹ç»“æœ
                last_token_logits = logits[0, -1, :]

                # [é‡‡æ ·é€»è¾‘]
                if temperature < 1e-5:
                    # è´ªå©ªæœç´¢ (Argmax): æ€»æ˜¯é€‰æ¦‚ç‡æœ€å¤§çš„
                    next_token = torch.argmax(last_token_logits).item()
                else:
                    # éšæœºé‡‡æ · (Multinomial): æ ¹æ®æ¦‚ç‡åˆ†å¸ƒæŠ½ç­¾
                    probs = torch.softmax(last_token_logits / temperature, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1).item()

                # é‡åˆ°ç»“æŸç¬¦ (EOS) åœæ­¢
                if next_token == sp.eos_id():
                    break
                
                # è§£ç å¹¶æ‰“å°
                word = sp.decode_ids([next_token])
                print(word, end="", flush=True)
                new_tokens_count += 1

                # [é‡è¦] å‡†å¤‡ä¸‹ä¸€æ¬¡è¾“å…¥
                # å› ä¸ºç”¨äº† KV Cacheï¼Œæˆ‘ä»¬åªéœ€è¦ä¼ è¿™ä¸€ä¸ªæ–° token
                # å½¢çŠ¶ä¿æŒ (1, 1)
                x = torch.tensor([[next_token]], dtype=torch.long, device=device)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        end_time = time.time()
        speed = new_tokens_count / (end_time - start_time)
        print(f"\n[Speed: {speed:.2f} token/s]\n")

    # ========================================================
    # 4. äº¤äº’å¼å¾ªç¯ (Control Loop)
    # ========================================================
    print("\nğŸ’¬ æ¬¢è¿ä½¿ç”¨ HappyLLM å¯¹è¯ç»ˆç«¯ï¼(è¾“å…¥ 'q' æˆ– 'exit' é€€å‡º)")
    print("-" * 50)
    
    while True:
        try:
            # è·å–ç”¨æˆ·è¾“å…¥
            user_input = input("\nUser: ")
            
            # é€€å‡ºæŒ‡ä»¤
            if user_input.lower() in ["q", "exit", "quit"]:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            # å¿½ç•¥ç©ºè¾“å…¥
            if not user_input.strip():
                continue
            
            # è°ƒç”¨èŠå¤©å‡½æ•°
            # è¿™é‡Œæ¸©åº¦è®¾ä¸º 0.8ï¼Œè®©å®ƒç¨å¾®æœ‰ç‚¹åˆ›é€ åŠ›
            chat(user_input, temperature=0.8)
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·å¼ºåˆ¶é€€å‡ºã€‚")
            break

if __name__ == "__main__":
    main()