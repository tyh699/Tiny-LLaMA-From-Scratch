import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from dataclasses import dataclass
from typing import Optional

# 1. é…ç½®ç±»
# ä½œç”¨ï¼šæŠŠæ¨¡å‹æ‰€æœ‰çš„è¶…å‚æ•°éƒ½æ”¾åœ¨ä¸€ä¸ªåœ°æ–¹æ–¹ä¾¿ç®¡ç†
@dataclass
class ModelArgs:
    dim: int = 4096 #æ¨¡å‹éšè—å±‚çš„ç»´åº¦
    n_layers: int = 32 #Transformer å±‚æ•°
    n_heads: int = 32 #å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
    n_kv_heads: Optional[int] = None #KV Cacheçš„å¤´æ•°ï¼ˆç”¨äºGQAï¼Œè‹¥ä¸ºNoneåˆ™ç­‰äº n_headsï¼‰
    vocab_size: int = -1 #è¯è¡¨å¤§å°ï¼ˆé€šå¸¸åœ¨åŠ è½½tokenizeråè®¾ç½®ï¼‰
    multiple_of: int = 256 #FFNéšè—å±‚ç»´åº¦çš„å€æ•°ï¼ˆç”¨äºSwiGLUç»´åº¦å¯¹é½ï¼‰
    ffn_dim_multiplier: Optional[float] = None #ç”¨äºå¾®è°ƒFFNä¸­é—´å±‚å¤§å°çš„ç³»æ•°
    norm_eps: float = 1e-5 #RMSNorm çš„epsilon é˜²æ­¢åˆ†æ¯ä¸º0
    max_seq_len: int = 2048 #æœ€å¤§åºåˆ—é•¿åº¦ ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼Œæ¨¡å‹ä¸€æ¬¡æœ€å¤šèƒ½çœ‹å¤šå°‘å­—
    dropout: float = 0.0 #Dropout æ¦‚ç‡ é˜²æ­¢è¿‡æ‹Ÿåˆçš„æœºåˆ¶ï¼Œè®­ç»ƒæ—¶éšæœºä¸¢å¼ƒä¸€äº›ç¥ç»å…ƒ


# 2.å½’ä¸€åŒ–å±‚ï¼ˆRMSNormï¼‰
# ä½œç”¨ï¼šè®©æ•°æ®åˆ†å¸ƒæ›´ç¨³å®šï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
# åŒºåˆ«ï¼šæ¯”ä¼ ç»Ÿçš„LayerNormå°‘å‡äº†ä¸€ä¸ªå‡å€¼Meanï¼Œè®¡ç®—æ›´å¿«ï¼Œæ•ˆæœå·®ä¸å¤š
class RMSNorm(nn.Module):
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        #è¿™æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è°ƒæ•´å®ƒæ¥æ”¾å¤§æˆ–ç¼©å°æ¨¡å‹
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self,x):
        # å…¬å¼ï¼š x*(1/sqrt(mean(x^2) + eps))  $$\bar{x}_i = \frac{x_i}{\sqrt{\frac{1}{n} \sum x_i^2 + \epsilon}}$$
        # x.pow(2):æ‰€æœ‰æ•°å¹³æ–¹
        # mean(-1):åœ¨æœ€åä¸€ä¸ªç»´åº¦æ±‚å‡å€¼
        # rsqrt:å¹³æ–¹æ ¹çš„å€’æ•°
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
    
    def forward(self,x):
        # æŠ€å·§ï¼šå…ˆè½¬æˆfloat32è¿›è¡Œè®¡ç®—ï¼ˆä¿è¯ç²¾åº¦ï¼‰ï¼Œç®—å®Œå†è½¬å›åŸæ¥çš„æ¨¡å‹ï¼ˆæ¯”å¦‚bfloat16ï¼‰
        output = self._norm(x.float()).type_as(x)
        # æœ€åä¹˜ä¸Šè¯¾å­¦ä¹ çš„ç¼©æ”¾å‚æ•°
        return output * self.weight
    

# 3.æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰è¾…åŠ©å‡½æ•°
# ä½œç”¨ï¼šå‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯åœ¨å¥å­é‡Œçš„ä½ç½®
# åŸç†ï¼šé€šè¿‡æ—‹è½¬å‘é‡çš„è§’åº¦æ¥è¡¨ç¤ºç›¸å¯¹ä½ç½®
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    # é¢„è®¡è®¡ç®—æ—‹è½¬è§’åº¦ï¼ˆå¤æ•°å½¢å¼ï¼‰ dimï¼šæ¯ä¸ªå¤´çš„ç»´åº¦ï¼ˆhead_dimï¼‰ endï¼šæœ€å¤§åºåˆ—é•¿åº¦ï¼ˆmax_seq_lenï¼‰
    # è®¡ç®—é¢‘ç‡ï¼š1/theta^(2i/dim)
    # è¿™é‡Œçš„åˆ‡ç‰‡ï¼š[: (dim // 2)] æ˜¯å› ä¸ºå¤æ•°éœ€è¦ä¸¤ä¸ªå®æ•°è¡¨ç¤ºï¼Œæ‰€ä»¥åªéœ€è¦ä¸€åŠçš„ç»´åº¦
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # ç”Ÿæˆä½ç½®ç´¢å¼•åºåˆ—ï¼š[0, 1, 2, ..., end-1]
    t = torch.arange(end, device=freqs.device)  # åˆ›å»ºä¸€ä¸ªé•¿åº¦ä¸ºendçš„åºåˆ—
    # å¤–ç§¯è®¡ç®—ï¼šç”Ÿæˆæ‰€æœ‰ä½ç½®å¯¹åº”çš„é¢‘ç‡
    # ç»“æœshapeï¼š(seq_len, dim / 2)
    freqs = torch.outer(t, freqs).float()  # çŸ©é˜µä¹˜æ³•

    # å°†æ¨¡é•¿è®¾ä¸º1ï¼Œè§’åº¦è®¾ä¸ºfreqsï¼Œç”Ÿæˆå¤æ•°(cos+i*sin)
    # ç»“æœæ˜¯ä¸€ä¸ªå¤æ•°å¼ é‡
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)
    return freqs_cis

def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):
    # å½¢çŠ¶è½¬æ¢ï¼šå°†æ—‹è½¬å‘é‡å¹¿æ’­åˆ°ä¸è¾“å…¥å¼ é‡çš„å½¢çŠ¶ä¸€è‡´
    # è°ƒæ•´é¢‘ç‡çŸ©é˜µçš„å½¢çŠ¶ï¼Œè®©ä»–èƒ½å’Œè¾“å…¥xè¿›è¡Œå¹¿æ’­ï¼ˆè‡ªåŠ¨å¯¹é½ï¼‰
    # ç›®æ ‡ï¼šè®©freqs_ciså˜æˆ(1,seq_len,head_dim/2)
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1],x.shape[-1])
    # æ„é€ æ–°å½¢çŠ¶ï¼šé™¤äº†seq_len å’Œ head_dim ç»´åº¦ï¼Œå…¶ä»–ç»´åº¦éƒ½è®¾ä¸º1
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)

def apply_rotary_emb(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
):
    # çœŸæ­£æ‰§è¡Œæ—‹è½¬æ“ä½œçš„åœ°æ–¹ï¼Œxqï¼šqueryå‘é‡ï¼Œxkï¼škeyå‘é‡ï¼Œfreqs_cisï¼šæ—‹è½¬è§’åº¦
    # æŠŠå®æ•°è½¬æˆå¤æ•°å½¢å¼ï¼Œæ¯”å¦‚shapeä»(...,dim)å˜æˆ(...,dim/2)ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¤æ•°
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1],-1,2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1],-1,2))

    # è°ƒæ•´é¢‘ç‡çŸ©é˜µå½¢çŠ¶ä»¥åŒ¹é…xq
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)

    # å¤æ•°ä¹˜æ³• = æ—‹è½¬æ“ä½œ
    # è¿™ä¸€æ­¥æŠŠä½ç½®ä¿¡æ¯æ³¨å…¥åˆ°äº†queryå’Œkeyä¸­
    # flatten(3)æ˜¯æŠŠå¤æ•°å†å±•å¹³å›å®æ•°ï¼š(...,dim/2) -> (...,dim)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)

    # è½¬å›åŸæ¥çš„æ•°æ®ç±»å‹è¿”å›
    return xq_out.type_as(xq), xk_out.type_as(xk)


# 4.æ³¨æ„åŠ›æœºåˆ¶ è¿™æ˜¯transformerçš„å¿ƒè„
class Attention(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        # å¤„ç†GQAï¼ˆåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼‰é€»è¾‘
        # å¦‚æœæ²¡è®¾ç½®n_kv_headsï¼Œå°±é»˜è®¤å’Œn_headsä¸€æ ·ï¼Œè¿™æ˜¯æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›
        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads
        self.n_heads = args.n_heads

        # è¿™é‡Œçš„localå˜é‡æ˜¯ä¸ºäº†å…¼å®¹å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œå½“å‰ä»£ç æˆ‘ä»¬åªç”¨å•å¡ï¼Œæ‰€ä»¥ç­‰äºtotal
        self.n_local_heads = args.n_heads
        self.n_local_kv_heads = self.n_kv_heads

        # è®¡ç®—æ¯ä¸ªå¤´çš„ç»´åº¦ ä¾‹å¦‚ dim=4096,heads=32ï¼Œé‚£ä¹ˆhead_dim=128
        self.head_dim = args.dim // args.n_heads

        # è®¡ç®—KVéœ€è¦é‡å¤å‡ æ¬¡ï¼Œæ¯”å¦‚Queryæœ‰32ä¸ªå¤´ï¼ŒKVåªæœ‰8ä¸ªå¤´ï¼Œé‚£ä¹ˆKVæ¯ä¸ªå¤´éœ€è¦é‡å¤4æ¬¡æ‰èƒ½åŒ¹é…
        self.n_rep = self.n_local_heads // self.n_local_kv_heads

        # å®šä¹‰4ä¸ªçº¿æ€§å±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰
        # ä¹Ÿå°±æ˜¯å…¬å¼é‡Œçš„Wq,Wk,Wv,Wo
        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False) # æ³¨æ„ç»´åº¦å¯èƒ½æ¯”Qå°
        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False) # è¾“å‡ºå±‚
        
        self.dropout = args.dropout

    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor] = None):
        # x.shape = (batch_size, seq_len, dim)
        bsz, seqlen, _ = x.shape

        # æŠ•å½±ï¼šæŠŠè¾“å…¥xå˜æˆQï¼ŒKï¼ŒV
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        # åˆ†å¤´ï¼šæŠŠé•¿å‘é‡åˆ‡æˆå¤šä¸ªå¤´
        # viewä¹‹åshapeï¼š(batch_size, seq_len, n_heads, head_dim)
        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

        # æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼šç»™Qå’ŒKåŠ ä¸Šä½ç½®ä¿¡æ¯ï¼Œæ³¨æ„ï¼šVä¸éœ€è¦åŠ ä½ç½®ç¼–ç 
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        # GQAå¤„ç†ï¼šå¦‚æœKVå¤´æ•°å°‘ï¼Œéœ€è¦å¤åˆ¶æ‰©å±•
        if self.n_rep > 1:
            # repeat_interleave: åœ¨dim=2ï¼ˆheadsç»´åº¦ï¼‰å¤åˆ¶n_repæ¬¡
            xk = torch.repeat_interleave(xk, self.n_rep, dim=2)
            xv = torch.repeat_interleave(xv, self.n_rep, dim=2)
        
        # è½¬ç½®ï¼šä¸ºäº†åšçŸ©é˜µä¹˜æ³•ï¼ŒæŠŠHeadsç§»åˆ°å‰é¢ shapeå˜æˆï¼š(Batch,Heads,Seq,Head_Dim)
        xq = xq.transpose(1, 2)
        xk = xk.transpose(1, 2)
        xv = xv.transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°scores
        # å…¬å¼ï¼šQ @ K.T / sqrt(dim)
        scores = torch.matmul(xq, xk.transpose(2, 3)) / math.sqrt(self.head_dim)

        # åº”ç”¨Mask
        # ä½œç”¨ï¼šè®©æ¨¡å‹çœ‹ä¸è§æœªæ¥çš„è¯ï¼ŒæŠŠæœªæ¥çš„ä½ç½®åˆ†æ•°è®¾ä¸ºè´Ÿæ— ç©·å¤§
        if mask is not None:
            scores = scores + mask
        
        # Softmaxå½’ä¸€åŒ–ï¼šæŠŠåˆ†æ•°å˜æˆæ¦‚ç‡ï¼Œå’Œä¸º1
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        scores = F.dropout(scores, p=self.dropout, training=self.training)

        # åŠ æƒæ±‚å’Œ ï¼šæŠŠæ¦‚ç‡ä¹˜ä»¥Vï¼Œå¾—åˆ°æ¯ä¸ªä½ç½®çš„è¾“å‡º
        output = torch.matmul(scores, xv)

        # è¿˜åŸå½¢çŠ¶ï¼ŒæŠŠ(Batch,Heads,Seq,Dim)å˜å›(Batch,Seq,Dim)
        # contiguousï¼šæŠŠå¼ é‡å˜æˆè¿ç»­çš„ï¼Œä¹Ÿå°±æ˜¯æŠŠå¼ é‡å˜æˆä¸€ä¸ªä¸€ç»´å¼ é‡ï¼Œå¦åˆ™æ— æ³•æ‰§è¡Œview
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)

        # æŠ•å½±ï¼šæŠŠè¾“å‡ºå˜æˆè¾“å‡ºå±‚
        return self.wo(output)
    

# 5.å‰é¦ˆç¥ç»ç½‘ç»œFeedForward
# ä½œç”¨ï¼šæ•´åˆä¿¡æ¯ï¼Œå¢åŠ éçº¿æ€§èƒ½åŠ›
# LLaMAç‰¹è‰²ï¼šä½¿ç”¨äº†SwiGLUæ¿€æ´»å‡½æ•°ï¼Œéœ€è¦ä¸‰ä¸ªçº¿æ€§å±‚
class FeedForward(nn.Module):
    def __init__(self, args: ModelArgs):
        super().__init__()
        # è®¡ç®—éšè—å±‚ç»´åº¦ï¼Œé€šå¸¸æ˜¯è¾“å…¥çš„4å€
        hidden_dim = 4 * args.dim
        hidden_dim = int(2 * hidden_dim / 3) # SwiGLUçš„ç‰¹æ®Šè°ƒæ•´

        # è°ƒæ•´hidden_dimæˆ–è€…æ˜¯256çš„å€æ•°ï¼Œä¸ºäº†ç¡¬ä»¶æ•ˆç‡
        if args.ffn_dim_multiplier is not None:
            hidden_dim = int(hidden_dim * args.ffn_dim_multiplier)
        hidden_dim = args.multiple_of * ((hidden_dim + args.multiple_of - 1) // args.multiple_of)

        # å®šä¹‰ä¸‰ä¸ªçº¿æ€§å±‚
        self.w1 = nn.Linear(args.dim, hidden_dim, bias=False) # é—¨æ§å±‚Gate
        self.w2 = nn.Linear(hidden_dim, args.dim, bias=False) # è¾“å‡ºå±‚Down
        self.w3 = nn.Linear(args.dim, hidden_dim, bias=False) # ç‰¹å¾å±‚Up

    def forward(self,x):
        # SwiGLUå…¬å¼ï¼šF.silu(w1(x)) * w3(x) -> å†è¿‡w2
        # siluå°±æ˜¯SiLUæ¿€æ´»å‡½æ•°ï¼Œè¿™é‡Œçš„ä¹˜æ³•æ˜¯é€å…ƒç´ ç›¸ä¹˜
        return self.w2(F.silu(self.w1(x)) * self.w3(x))
        
# 6.Transformerå±‚ TransformerBlock
# ä½œç”¨ï¼šæŠŠAttentionå’ŒFeedForwardè¿æ¥èµ·æ¥ï¼Œç»„æˆä¸€å±‚
class TransformerBlock(nn.Module):
    def __init__(self,layer_id: int, args: ModelArgs):
        super().__init__()
        self.n_heads = args.n_heads
        self.dim = args.dim
        self.head_dim = args.dim // args.n_heads

        # å®ä¾‹åŒ–å­æ¨¡å—
        self.attention = Attention(args)
        self.feed_forward = FeedForward(args)
        # æ¯ä¸ªå­æ¨¡å—å‰éƒ½æœ‰ä¸€ä¸ªNorm
        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)
        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)

    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        # æ®‹å·®è¿æ¥(Residal Connection): x = x + f(x)
        # å…ˆåšNormï¼Œå†åšAttentionï¼Œç»“æœåŠ å›x
        h = x + self.attention.forward(self.attention_norm(x), freqs_cis, mask)
        # å…ˆåšNormï¼Œå†åšFeedForwardï¼Œç»“æœåŠ å›h
        out = h + self.feed_forward.forward(self.ffn_norm(h))
        return out
    
# 7.Transformeræ¨¡å‹
# ä½œç”¨ï¼šæ­ç§¯æœ¨ï¼ŒæŠŠEmbeddingï¼Œ32å±‚Blockï¼Œè¾“å‡ºå±‚ç»„è£…åœ¨ä¸€èµ·
class Transformer(nn.Module):
    def __init__(self,params: ModelArgs):
        super().__init__()
        self.params = params
        self.vocab_size = params.vocab_size
        self.n_layers = params.n_layers

        # è¯åµŒå…¥å±‚ï¼šæŠŠToken IDå˜æˆå‘é‡
        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)

        # å †å Nå±‚TransformerBlock
        self.layers = nn.ModuleList()
        for layer_id in range(params.n_layers):
            self.layers.append(TransformerBlock(layer_id, params))
        
        # æœ€ç»ˆçš„å½’ä¸€åŒ–å±‚
        self.norm = RMSNorm(params.dim, eps=params.norm_eps)

        # è¾“å‡ºå±‚ï¼šæŠŠå‘é‡å˜å›è¯è¡¨æ¦‚ç‡
        self.output = nn.Linear(params.dim, params.vocab_size, bias=False)

        # ä¼˜åŒ–ç‚¹ï¼šä¸è®¡ç®—RoPEæ—‹è½¬çŸ©é˜µ
        # register_bufferå‘Šè¯‰pytorchï¼šè¿™æ˜¯æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä½†ä¸æ˜¯éœ€è¦æ›´æ–°çš„å‚æ•°
        # è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼šå½“ä½ model.to("cuda")æ—¶ï¼Œè¿™äº›æ•°æ®ä¼šè‡ªåŠ¨è·Ÿç€å»æ˜¾å¡ï¼Œä¸ç”¨æ“å¿ƒ
        freqs_cis = precompute_freqs_cis(params.dim // params.n_heads, params.max_seq_len * 2)
        self.register_buffer("freqs_cis", freqs_cis, persistent=False)

    def forward(self, tokens: torch.Tensor, start_pos: int = 0):
        # tokens shape: (Batch, Seq_Len)
        bsz, seqlen = tokens.shape

        # æŸ¥è¡¨ï¼šIDå˜æˆå‘é‡
        h = self.tok_embeddings(tokens)
        
        # è·å–å¯¹åº”çš„RePE æ—‹è½¬çŸ©é˜µ
        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]

        # ç”ŸæˆMask
        # ç›®æ ‡ï¼šç”Ÿæˆä¸€ä¸ªä¸Šä¸‰è§’å…¨æ˜¯è´Ÿæ— ç©·çš„çŸ©é˜µ
        mask = None
        if seqlen > 1:
            mask = torch.full((seqlen, seqlen), float("-inf"), device=tokens.device)
            mask = torch.triu(mask, diagonal=1) # ä¿ç•™ä¸Šä¸‰è§’ï¼Œå¯¹è§’çº¿åç§»1

            # ä¸ºäº†å¤„ç†start_pos(æ¨ç†æ—¶çš„ç¼“å­˜)ï¼Œå¯èƒ½éœ€è¦æ¨ªå‘æ‰©å±•mask
            mask = torch.hstack([torch.zeros((seqlen,start_pos), device=tokens.device),mask]).type_as(h)

        # ä¸€å±‚å±‚æµè¿‡TransformerBlocks
        for layer in self.layers:
            h = layer(h, freqs_cis, mask)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        h = self.norm(h)

        # æ˜ å°„å›è¯è¡¨å¤§å°ï¼Œå¾—åˆ°Logits
        output = self.output(h).float()
        return output

# ==========================================
# å¢å¼ºç‰ˆéªŒè¯ä»£ç  (Forward + Backward)
# ä½œç”¨ï¼šæµ‹è¯•æ¨¡å‹èƒ½ä¸èƒ½è·‘é€šï¼Œèƒ½ä¸èƒ½å­¦ä¹ ï¼ˆæœ‰æ¢¯åº¦ï¼‰ã€‚
# ==========================================
if __name__ == "__main__":
    # 1. æ£€æŸ¥æœ‰æ²¡æœ‰æ˜¾å¡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨è®¡ç®—è®¾å¤‡: {device.upper()}")

    # 2. å®šä¹‰æµ‹è¯•å‚æ•° 
    # (ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æŠŠæ¨¡å‹è®¾å¾—å¾ˆå°ï¼Œé˜²æ­¢ä½ ç”µè„‘å¡æ­»)
    args = ModelArgs(
        dim=512,          # æ­£å¸¸æ˜¯ 4096ï¼Œè¿™é‡Œç¼©åˆ° 512
        n_layers=4,       # æ­£å¸¸æ˜¯ 32ï¼Œè¿™é‡Œåªç”¨ 4 å±‚
        n_heads=8,        
        vocab_size=5000, 
        max_seq_len=128
    )
    
    # 3. åˆå§‹åŒ–æ¨¡å‹å¹¶æ¬åˆ° GPU
    print("ğŸ› ï¸  æ­£åœ¨åˆå§‹åŒ– LLaMA æ¶æ„æ¨¡å‹...")
    model = Transformer(args).to(device)
    
    # è®¡ç®—å‚æ•°é‡
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°é‡: {total_params / 1e6:.2f} Million")

    # 4. æ„é€ è™šæ‹Ÿæ•°æ® (éšæœºç”Ÿæˆçš„æ•°å­—)
    batch_size = 4
    seq_len = 32
    print(f"ğŸ“¥ è¾“å…¥å½¢çŠ¶: (Batch={batch_size}, Seq={seq_len})")
    
    inputs = torch.randint(0, args.vocab_size, (batch_size, seq_len)).to(device)
    targets = torch.randint(0, args.vocab_size, (batch_size, seq_len)).to(device) # å‡è£…è¿™æ˜¯æ­£ç¡®ç­”æ¡ˆ

    # ==========================
    # éªŒè¯ A: å‰å‘ä¼ æ’­ (Forward)
    # ==========================
    print("\nğŸ”„ [Step 1] æµ‹è¯•å‰å‘ä¼ æ’­ (Forward)...")
    try:
        logits = model(inputs)
        print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {logits.shape}")
        # æ£€æŸ¥è¾“å‡ºç»´åº¦æ˜¯ä¸æ˜¯ (B, L, Vocab_Size)
        assert logits.shape == (batch_size, seq_len, args.vocab_size)
    except Exception as e:
        print(f"âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        exit()

    # ==========================
    # éªŒè¯ B: åå‘ä¼ æ’­ (æ¢¯åº¦æ£€æŸ¥)
    # ==========================
    print("\nğŸ”„ [Step 2] æµ‹è¯•åå‘ä¼ æ’­ (Backward)...")
    try:
        # è®¡ç®— Loss
        # view(-1, ...) æ˜¯æŠŠæ•°æ®æ‹‰å¹³æˆä¸€é•¿æ¡ï¼Œè¿™æ˜¯ CrossEntropyLoss è¦æ±‚çš„æ ¼å¼
        loss = F.cross_entropy(logits.view(-1, args.vocab_size), targets.view(-1))
        print(f"ğŸ“‰ å½“å‰ Loss: {loss.item():.4f}")
        
        # åå‘ä¼ æ’­ (æ±‚å¯¼)
        loss.backward()
        
        # æ£€æŸ¥ç¬¬ä¸€å±‚ (Embedding) æœ‰æ²¡æœ‰æ”¶åˆ°æ¢¯åº¦
        # å¦‚æœ grad ä¸æ˜¯ None ä¸” norm > 0ï¼Œè¯´æ˜ç½‘ç»œæ˜¯é€šçš„ï¼
        grad_norm = model.tok_embeddings.weight.grad.norm().item()
        print(f"âœ… åå‘ä¼ æ’­æˆåŠŸï¼æ¢¯åº¦å·²ç”Ÿæˆã€‚")
        print(f"ğŸ” Token Embedding å±‚æ¢¯åº¦èŒƒæ•°: {grad_norm:.4f}")
        
        if grad_norm > 0:
            print("\nğŸ‰ æ­å–œï¼æ¨¡å‹å¤ç°æˆåŠŸï¼Œä¸”å…·å¤‡å­¦ä¹ èƒ½åŠ›ï¼")
        else:
            print("\nâš ï¸ è­¦å‘Šï¼šæ¢¯åº¦ä¸º 0ï¼Œå¯èƒ½å­˜åœ¨æ–­é“¾ã€‚")
            
    except Exception as e:
        print(f"âŒ åå‘ä¼ æ’­å¤±è´¥: {e}")